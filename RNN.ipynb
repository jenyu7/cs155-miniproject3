{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sonnet:\n",
    "    def __init__(self):\n",
    "        self.lines = []\n",
    "\n",
    "    def add_line(self, line):\n",
    "        self.lines.append(line)\n",
    "        \n",
    "    def print_lines(self):\n",
    "        for line in self.lines:\n",
    "            print(line)\n",
    "            \n",
    "    def get_lines(self):\n",
    "        return self.lines\n",
    "            \n",
    "    # Returns the tokenization of the entire poem as a list of\n",
    "    # all the words in the sonnet\n",
    "    def get_poem_tokenization(self):\n",
    "        tokenized = []\n",
    "        for line in self.lines:\n",
    "            tokenized.extend(line.split(' '))\n",
    "            \n",
    "        return tokenized\n",
    "    \n",
    "    # Returns the tokenization of the stanzas in the sonnet,\n",
    "    # where each element in the returned list corresponds to\n",
    "    # the tokenization of the corresponding stanza\n",
    "    def get_stanza_tokenization(self):\n",
    "        tokenized = []\n",
    "        \n",
    "        stanzas = [self.lines[0:4], \n",
    "                   self.lines[4:8], \n",
    "                   self.lines[8:12],\n",
    "                   self.lines[12:14]]\n",
    "        \n",
    "        for i, lines in enumerate(stanzas):\n",
    "            tokenized.append([])\n",
    "            for line in lines:\n",
    "                tokenized[i].extend(line.split(' '))\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Returns the tokenization of the lines in the sonnet,\n",
    "    # where each element in the returned list corresponds to\n",
    "    # the tokenization of the corresponding line\n",
    "    def get_line_tokenization(self):\n",
    "        tokenized = []\n",
    "        \n",
    "        for line in self.lines:\n",
    "            tokenized.append(line.split(' '))\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "# Takes in a string and strips the leading/trailing whitespace\n",
    "#, makes the string lowercase, and removes all punctuation\n",
    "def prettify(line):\n",
    "    punctuation = ['.', ',', ':', ';', '!', '?']\n",
    "    line = line.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    #for c in punctuation:\n",
    "    #    line = line.replace(c, '')\n",
    "        \n",
    "    return line\n",
    "\n",
    "# List of all sonnets contained in data/shakespeare.txt\n",
    "sonnets = []\n",
    "\n",
    "f = open(\"data/shakespeare.txt\", \"r\")\n",
    "\n",
    "line = f.readline()\n",
    "while line:\n",
    "    # Create the new sonnet object and add it to the list\n",
    "    sonnet = Sonnet()\n",
    "    for i in range(14):\n",
    "        line = prettify(f.readline())\n",
    "        sonnet.add_line(line)\n",
    "    \n",
    "    sonnets.append(sonnet)\n",
    "    \n",
    "    # Skip over the space in between sonnets\n",
    "    for i in range(3):\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "94584/94584 [==============================] - 35s 365us/step - loss: 2.0078\n",
      "Epoch 2/60\n",
      "94584/94584 [==============================] - 37s 391us/step - loss: 1.6296\n",
      "Epoch 3/60\n",
      "94584/94584 [==============================] - 37s 387us/step - loss: 1.5189\n",
      "Epoch 4/60\n",
      "94584/94584 [==============================] - 37s 394us/step - loss: 1.4528\n",
      "Epoch 5/60\n",
      "94584/94584 [==============================] - 37s 394us/step - loss: 1.4042\n",
      "Epoch 6/60\n",
      "94584/94584 [==============================] - 37s 393us/step - loss: 1.3731\n",
      "Epoch 7/60\n",
      "94584/94584 [==============================] - 37s 387us/step - loss: 1.3457\n",
      "Epoch 8/60\n",
      "94584/94584 [==============================] - 35s 370us/step - loss: 1.3201\n",
      "Epoch 9/60\n",
      "94584/94584 [==============================] - 31s 332us/step - loss: 1.3019\n",
      "Epoch 10/60\n",
      "94584/94584 [==============================] - 31s 333us/step - loss: 1.2837\n",
      "Epoch 11/60\n",
      "94584/94584 [==============================] - 32s 333us/step - loss: 1.2690\n",
      "Epoch 12/60\n",
      "94584/94584 [==============================] - 31s 332us/step - loss: 1.2562\n",
      "Epoch 13/60\n",
      "94584/94584 [==============================] - 31s 330us/step - loss: 1.2453\n",
      "Epoch 14/60\n",
      "94584/94584 [==============================] - 31s 330us/step - loss: 1.2350\n",
      "Epoch 15/60\n",
      "94584/94584 [==============================] - 31s 329us/step - loss: 1.2272\n",
      "Epoch 16/60\n",
      "94584/94584 [==============================] - 31s 332us/step - loss: 1.2187\n",
      "Epoch 17/60\n",
      "94584/94584 [==============================] - 32s 342us/step - loss: 1.2118\n",
      "Epoch 18/60\n",
      "94584/94584 [==============================] - 31s 329us/step - loss: 1.2065\n",
      "Epoch 19/60\n",
      "94584/94584 [==============================] - 31s 333us/step - loss: 1.2009\n",
      "Epoch 20/60\n",
      "94584/94584 [==============================] - 32s 334us/step - loss: 1.1946\n",
      "Epoch 21/60\n",
      "94584/94584 [==============================] - 32s 334us/step - loss: 1.1892\n",
      "Epoch 22/60\n",
      "94584/94584 [==============================] - 32s 339us/step - loss: 1.1809\n",
      "Epoch 23/60\n",
      "94584/94584 [==============================] - 31s 330us/step - loss: 1.1765\n",
      "Epoch 24/60\n",
      "94584/94584 [==============================] - 32s 336us/step - loss: 1.1699\n",
      "Epoch 25/60\n",
      "94584/94584 [==============================] - 31s 333us/step - loss: 1.1699\n",
      "Epoch 26/60\n",
      "94584/94584 [==============================] - 34s 355us/step - loss: 1.1635\n",
      "Epoch 27/60\n",
      "94584/94584 [==============================] - 31s 326us/step - loss: 1.1568\n",
      "Epoch 28/60\n",
      "94584/94584 [==============================] - 31s 326us/step - loss: 1.1553\n",
      "Epoch 29/60\n",
      "94584/94584 [==============================] - 31s 328us/step - loss: 1.1541\n",
      "Epoch 30/60\n",
      "94584/94584 [==============================] - 31s 326us/step - loss: 1.1521\n",
      "Epoch 31/60\n",
      "94584/94584 [==============================] - 31s 327us/step - loss: 1.1462\n",
      "Epoch 32/60\n",
      "94584/94584 [==============================] - 31s 327us/step - loss: 1.1429\n",
      "Epoch 33/60\n",
      "94584/94584 [==============================] - 31s 326us/step - loss: 1.1383\n",
      "Epoch 34/60\n",
      "94584/94584 [==============================] - 31s 327us/step - loss: 1.1388\n",
      "Epoch 35/60\n",
      "94584/94584 [==============================] - 31s 328us/step - loss: 1.1323\n",
      "Epoch 36/60\n",
      "94584/94584 [==============================] - 33s 346us/step - loss: 1.1286\n",
      "Epoch 37/60\n",
      "94584/94584 [==============================] - 31s 327us/step - loss: 1.1255\n",
      "Epoch 38/60\n",
      "94584/94584 [==============================] - 31s 328us/step - loss: 1.1192\n",
      "Epoch 39/60\n",
      "94584/94584 [==============================] - 31s 329us/step - loss: 1.1310\n",
      "Epoch 40/60\n",
      "94584/94584 [==============================] - 31s 329us/step - loss: 1.1290\n",
      "Epoch 41/60\n",
      "94584/94584 [==============================] - 31s 328us/step - loss: 1.1207\n",
      "Epoch 42/60\n",
      "94584/94584 [==============================] - 31s 328us/step - loss: 1.1169\n",
      "Epoch 43/60\n",
      "94584/94584 [==============================] - 31s 329us/step - loss: 1.1095\n",
      "Epoch 44/60\n",
      "94584/94584 [==============================] - 32s 335us/step - loss: 1.1085\n",
      "Epoch 45/60\n",
      "94584/94584 [==============================] - 31s 331us/step - loss: 1.1020\n",
      "Epoch 46/60\n",
      "94584/94584 [==============================] - 31s 329us/step - loss: 1.0987\n",
      "Epoch 47/60\n",
      "94584/94584 [==============================] - 31s 328us/step - loss: 1.0980\n",
      "Epoch 48/60\n",
      "94584/94584 [==============================] - 31s 331us/step - loss: 1.0932\n",
      "Epoch 49/60\n",
      "94584/94584 [==============================] - 38s 404us/step - loss: 1.0916\n",
      "Epoch 50/60\n",
      "94584/94584 [==============================] - 40s 418us/step - loss: 1.0885\n",
      "Epoch 51/60\n",
      "94584/94584 [==============================] - 38s 400us/step - loss: 1.0847\n",
      "Epoch 52/60\n",
      "94584/94584 [==============================] - 39s 409us/step - loss: 1.0838\n",
      "Epoch 53/60\n",
      "94584/94584 [==============================] - 38s 403us/step - loss: 1.0823\n",
      "Epoch 54/60\n",
      "94584/94584 [==============================] - 39s 412us/step - loss: 1.0796\n",
      "Epoch 55/60\n",
      "94584/94584 [==============================] - 33s 344us/step - loss: 1.0763\n",
      "Epoch 56/60\n",
      "94584/94584 [==============================] - 39s 417us/step - loss: 1.0774\n",
      "Epoch 57/60\n",
      "94584/94584 [==============================] - 34s 363us/step - loss: 1.0728\n",
      "Epoch 58/60\n",
      "94584/94584 [==============================] - 32s 342us/step - loss: 1.0701\n",
      "Epoch 59/60\n",
      "94584/94584 [==============================] - 36s 378us/step - loss: 1.0655\n",
      "Epoch 60/60\n",
      "94584/94584 [==============================] - 37s 394us/step - loss: 1.0654\n"
     ]
    }
   ],
   "source": [
    "def get_all_lines():\n",
    "    lines = []\n",
    "    for sonnet in sonnets:\n",
    "        lines.extend(sonnet.get_lines())\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def create_model():\n",
    "    lines = get_all_lines()\n",
    "    chars = sorted(set([c for line in lines for c in line]))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(40, len(chars))))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, epochs=100):\n",
    "    lines = get_all_lines()\n",
    "    chars = sorted(set([c for line in lines for c in line]))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    full_text = ' '.join([line for line in lines])\n",
    "\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(len(full_text) - 40):\n",
    "        sequences.append(full_text[i: i + 40])\n",
    "        next_chars.append(full_text[i + 40])\n",
    "    \n",
    "    x = np.zeros((len(sequences), 40, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j, char in enumerate(sequence):\n",
    "            x[i, j, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "        \n",
    "    model.fit(x, y, batch_size=128, epochs=epochs)\n",
    "\n",
    "model = create_model()\n",
    "train_model(model, epochs=60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('------ temperature: ', 0.2)\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      " when thou art the world of the truth, thee she dost brow,\n",
      " and then the world should more than thy self bright,\n",
      " and there is not thee i cannot the truth.\n",
      " \n",
      " which she lov's which thou with"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " thee thy love,\n",
      " which thou art with thee thee i am thee,\n",
      "   then whose brief farses and there thou art,\n",
      " and they had thee more breast with thee despited,\n",
      " and thence thee she thought it is not face,\n",
      " and ther()\n",
      "('------ temperature: ', 0.5)\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      " when thou art hath thou beauty seemed thee,\n",
      "   then then were it when i am not and me kind,\n",
      " steeatchoun in hie advantage is so surve.\n",
      " but the glory with the wired, which live erred\n",
      " my say thy self all my heart doth love's sweet self death,\n",
      " and made to thee i am new fiels come,\n",
      " but the heart and my will corrupion thee shines more,\n",
      " and with thee the world with golden tears,\n",
      " and look it is be()\n",
      "('------ temperature: ', 1.0)\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      " in cheeks feed, but sweetes of thee may,\n",
      " crown and pity now bobbly things might,\n",
      " that in these offence hath mine is happy,\n",
      " and so sun of sif,\n",
      " thane him declough, their viass'st will one his wit.\n",
      "   is not from far the dear to thee their now,\n",
      "   'tis ydent antique more woild shall fears,\n",
      " and in happies that that plea count, and cheeks again,\n",
      " more to flame ofcearded not proch dith none,\n",
      " in o()\n",
      "('------ temperature: ', 1)\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      " when none for love is better autup men's,\n",
      " - is got me fearfully mad me's ma.\n",
      " now is blint i cannot begint of love brights.\n",
      " \n",
      " nor rights ne'er-cloud straight, he amen,\n",
      " against thy glowy, thee in your eto thee.\n",
      " i do the dear heart thy thought is prises,\n",
      " and then sweets in his mawn out of garmed's figuthers of me,\n",
      "   yet unloods, and thy deserving them still,\n",
      " but some lines himkey, opprisuing()\n",
      "('------ temperature: ', 2)\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      " nightunzey, \n",
      " hicking -pathorn, and ligened, ykff-juntdest\n",
      " soer you i forpuny to brimened, \n",
      " cenus errunks vexiut,\n",
      " and geitues douo'erss abnance, but slut-hed,\n",
      " it leaks what suso-ble, pipef giveldsp,\n",
      " caulls enform stcalfusmex numy.\n",
      " what from toning indirerta?\n",
      " yquey ob wald of love can osuse enjubllckb,\n",
      " bythet i .oberdien morany poty find lever\n",
      "agary are lord are dhrowns forn) u'tind,\n",
      " geve()\n"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "    \n",
    "\n",
    "def generate_sonnet():\n",
    "    lines = get_all_lines()\n",
    "    chars = sorted(set([c for line in lines for c in line]))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    full_text = ' '.join([line for line in lines])\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1,2]:\n",
    "        print('------ temperature: ', diversity)\n",
    "        \n",
    "        generated = ''\n",
    "        sentence = prettify(\"shall I compare thee to a summer's day?\\n\")\n",
    "        generated += sentence\n",
    "        \n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "        \n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, 40, len(chars)))\n",
    "            for j, char in enumerate(sentence):\n",
    "                x_pred[0, j, char_indices[char]] = 1.0\n",
    "            \n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            \n",
    "            sentence = sentence[1:] + next_char\n",
    "            \n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "generate_sonnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
